{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# NLU Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from ramble.on import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#? config\n",
    "cfg['required_files'].append('model.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting model.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile model.py\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "\"\"\"\n",
    "O código original deste arquivo está em https://github.com/dennybritz/cnn-text-classification-tf/blob/master/text_cnn.py\n",
    "e foi adaptado por Peterson Katagiri Zilli <peterson.zilli@gmail.com>\n",
    "\"\"\"\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "class TextCNN(object):\n",
    "    \"\"\"\n",
    "    A CNN for text classification.\n",
    "    Uses an embedding layer, followed by a convolutional, max-pooling and softmax layer.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "      self, sequence_length, num_classes, vocab_size,\n",
    "      embedding_size, filter_sizes, num_filters, l2_reg_lambda=0.0):\n",
    "\n",
    "        # Placeholders for input, output and dropout\n",
    "        self.input_x = tf.placeholder(tf.int32, [None, sequence_length], name=\"input_x\")\n",
    "        self.input_y = tf.placeholder(tf.float32, [None, num_classes], name=\"input_y\")\n",
    "        self.dropout_keep_prob = tf.placeholder(tf.float32, name=\"dropout_keep_prob\")\n",
    "\n",
    "        # Keeping track of l2 regularization loss (optional)\n",
    "        l2_loss = tf.constant(0.0)\n",
    "\n",
    "        # Embedding layer\n",
    "        with tf.device('/cpu:0'), tf.name_scope(\"embedding\"):\n",
    "            self.W = tf.Variable(\n",
    "                tf.random_uniform([vocab_size, embedding_size], -1.0, 1.0),\n",
    "                name=\"W\")\n",
    "            self.embedded_chars = tf.nn.embedding_lookup(self.W, self.input_x)\n",
    "            self.embedded_chars_expanded = tf.expand_dims(self.embedded_chars, -1)\n",
    "\n",
    "        # Create a convolution + maxpool layer for each filter size\n",
    "        pooled_outputs = []\n",
    "        for i, filter_size in enumerate(filter_sizes):\n",
    "            with tf.name_scope(\"conv-maxpool-%s\" % filter_size):\n",
    "                # Convolution Layer\n",
    "                filter_shape = [filter_size, embedding_size, 1, num_filters]\n",
    "                W = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.1), name=\"W\")\n",
    "                b = tf.Variable(tf.constant(0.1, shape=[num_filters]), name=\"b\")\n",
    "                conv = tf.nn.conv2d(\n",
    "                    self.embedded_chars_expanded,\n",
    "                    W,\n",
    "                    strides=[1, 1, 1, 1],\n",
    "                    padding=\"VALID\",\n",
    "                    name=\"conv\")\n",
    "                # Apply nonlinearity\n",
    "                h = tf.nn.relu(tf.nn.bias_add(conv, b), name=\"relu\")\n",
    "                # Maxpooling over the outputs\n",
    "                pooled = tf.nn.max_pool(\n",
    "                    h,\n",
    "                    ksize=[1, sequence_length - filter_size + 1, 1, 1],\n",
    "                    strides=[1, 1, 1, 1],\n",
    "                    padding='VALID',\n",
    "                    name=\"pool\")\n",
    "                pooled_outputs.append(pooled)\n",
    "\n",
    "        # Combine all the pooled features\n",
    "        num_filters_total = num_filters * len(filter_sizes)\n",
    "        self.h_pool = tf.concat(pooled_outputs, 3)\n",
    "        self.h_pool_flat = tf.reshape(self.h_pool, [-1, num_filters_total])\n",
    "\n",
    "        # Add dropout\n",
    "        with tf.name_scope(\"dropout\"):\n",
    "            self.h_drop = tf.nn.dropout(self.h_pool_flat, self.dropout_keep_prob)\n",
    "\n",
    "        # Final (unnormalized) scores and predictions\n",
    "        with tf.name_scope(\"output\"):\n",
    "            W = tf.get_variable(\n",
    "                \"W\",\n",
    "                shape=[num_filters_total, num_classes],\n",
    "                initializer=tf.contrib.layers.xavier_initializer())\n",
    "            b = tf.Variable(tf.constant(0.1, shape=[num_classes]), name=\"b\")\n",
    "            l2_loss += tf.nn.l2_loss(W)\n",
    "            l2_loss += tf.nn.l2_loss(b)\n",
    "            self.scores = tf.nn.xw_plus_b(self.h_drop, W, b, name=\"scores\")\n",
    "            self.predictions = tf.argmax(self.scores, 1, name=\"predictions\")\n",
    "\n",
    "        # CalculateMean cross-entropy loss\n",
    "        with tf.name_scope(\"loss\"):\n",
    "            losses = tf.nn.softmax_cross_entropy_with_logits(logits=self.scores, labels=self.input_y)\n",
    "            self.loss = tf.reduce_mean(losses) + l2_reg_lambda * l2_loss\n",
    "\n",
    "        # Accuracy\n",
    "        with tf.name_scope(\"accuracy\"):\n",
    "            correct_predictions = tf.equal(self.predictions, tf.argmax(self.input_y, 1))\n",
    "            self.accuracy = tf.reduce_mean(tf.cast(correct_predictions, \"float\"), name=\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting data_helpers.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile data_helpers.py\n",
    "# -*- coding: utf-8 -*-\n",
    "# baseado no original em: https://github.com/dennybritz/cnn-text-classification-tf/blob/master/data_helpers.py\n",
    "\n",
    "import re\n",
    "import codecs\n",
    "import numpy as np\n",
    "\n",
    "def clean_str(string):\n",
    "    \"\"\"\n",
    "    Tokenização/limpeza do dataset baseado no trabalho do yoonkin\n",
    "    em https://github.com/yoonkim/CNN_sentence/blob/master/process_data.py\n",
    "    \"\"\"\n",
    "    # tratamento dos caracteres especiais em português\n",
    "    string = re.sub(r\"ç\", \"c\", string)\n",
    "    string = re.sub(r\"ã\", \"a\", string)\n",
    "    string = re.sub(r\"á\", \"a\", string)\n",
    "    string = re.sub(r\"à\", \"a\", string)\n",
    "    string = re.sub(r\"â\", \"a\", string)\n",
    "    string = re.sub(r\"é\", \"e\", string)\n",
    "    string = re.sub(r\"ê\", \"e\", string)\n",
    "    string = re.sub(r\"í\", \"i\", string)\n",
    "    string = re.sub(r\"õ\", \"o\", string)\n",
    "    string = re.sub(r\"ó\", \"o\", string)\n",
    "    string = re.sub(r\"ô\", \"o\", string)\n",
    "    string = re.sub(r\"ú\", \"u\", string)\n",
    "\n",
    "    # substitui tudo que não for esses caracteres abaixo por espaço\n",
    "    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)    \n",
    "\n",
    "    # parte do treinamento original do yoonkin para inglês    \n",
    "    string = re.sub(r\"\\'s\", \" \\'s\", string)\n",
    "    string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n",
    "    string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n",
    "    string = re.sub(r\"\\'re\", \" \\'re\", string)\n",
    "    string = re.sub(r\"\\'d\", \" \\'d\", string)\n",
    "    string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n",
    "\n",
    "    # tratamento da pontuação\n",
    "    string = re.sub(r\",\", \" , \", string)\n",
    "    string = re.sub(r\"!\", \" ! \", string)\n",
    "    string = re.sub(r\"\\(\", \" \\( \", string)\n",
    "    string = re.sub(r\"\\)\", \" \\) \", string)\n",
    "    string = re.sub(r\"\\\"\", \" \\\" \", string)\n",
    "    string = re.sub(r\"\\?\", \" \\? \", string)\n",
    "    \n",
    "    # tratamento dos espaços duplicados\n",
    "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
    "    \n",
    "    return string.strip().lower()\n",
    "\n",
    "def compact_str(string):\n",
    "    # Detecção: oi, oooi, ooooieeeee, olaaa, olar\n",
    "    string = re.sub(r\"^(o+i+e*|o+l+a+r*)\", \"oie\", string)\n",
    "    # Detecção: hahahahaha, heheheh, kkkk, rsrs\n",
    "    string = re.sub(r\"([aei]?(h[aei]){2,}h?|k{3,})|(rs){2,}\", \"hahaha\", string)\n",
    "    # Detecção: nooooossa --> nossa (mais de 2 caracteres consecutivos viram 1 só)\n",
    "    # Resultado aparentemente insatisfatório.\n",
    "    #string = re.sub(r\"(.)\\1{2,}\", \"$1\", string)\n",
    "    return string\n",
    "\n",
    "def gen_label(size, index):\n",
    "    \"\"\"\n",
    "    Gera uma lista de tamanho 'size' toda de zeros, com excessão de colocar um '1' na posição index\n",
    "    \"\"\"\n",
    "    aux = [0] * size\n",
    "    aux[index] = 1\n",
    "    return aux\n",
    "    \n",
    "def load_data_and_labels(data_folder):\n",
    "    \"\"\"\n",
    "    Carrega os dados de arquivos de dados na pasta, faz o split dos textos em palavras e gera os labels\n",
    "    Baseado no original em: https://github.com/dennybritz/cnn-text-classification-tf/blob/master/data_helpers.py\n",
    "    Retorna as frases splitadas e os labels\n",
    "    \"\"\"\n",
    "\n",
    "    import os\n",
    "    files = [file for file in os.listdir(data_folder) if file.lower().endswith(\".txt\")]\n",
    "    num_files = len(files)\n",
    "\n",
    "    x_text = []\n",
    "    y = []\n",
    "\n",
    "    for index_file, file in enumerate(files):\n",
    "        text_examples = list(codecs.open(os.path.join(data_folder, file), \"r\", \"utf-8\").readlines())\n",
    "        text_examples = [compact_str(clean_str(s)).strip() for s in text_examples]\n",
    "        text_labels = [gen_label(num_files, index_file) for _ in text_examples]\n",
    "        x_text += text_examples\n",
    "        y += text_labels\n",
    "        \n",
    "    y = np.array(y)\n",
    "\n",
    "    return [files, x_text, y]\n",
    "\n",
    "\n",
    "def batch_iter(data, batch_size, num_epochs, shuffle=True):\n",
    "    \"\"\"\n",
    "    Generates a batch iterator for a dataset.\n",
    "    \"\"\"\n",
    "    data = np.array(data)\n",
    "    data_size = len(data)\n",
    "    num_batches_per_epoch = int((len(data)-1)/batch_size) + 1\n",
    "    for epoch in range(num_epochs):\n",
    "        # Shuffle the data at each epoch\n",
    "        if shuffle:\n",
    "            shuffle_indices = np.random.permutation(np.arange(data_size))\n",
    "            shuffled_data = data[shuffle_indices]\n",
    "        else:\n",
    "            shuffled_data = data\n",
    "        for batch_num in range(num_batches_per_epoch):\n",
    "            start_index = batch_num * batch_size\n",
    "            end_index = min((batch_num + 1) * batch_size, data_size)\n",
    "            yield shuffled_data[start_index:end_index]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile train.py\n",
    "# -*- coding: utf-8 -*-\n",
    "# !/usr/bin/env python\n",
    "\n",
    "\"\"\"\n",
    "Adaptado de https://github.com/dennybritz/cnn-text-classification-tf/blob/master/train.py\n",
    "por Peterson Katagiri Zilli <peterson.zilli@gmail.com>\n",
    "\"\"\"\n",
    "\n",
    "import datetime\n",
    "import os\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import learn\n",
    "\n",
    "import data_helpers\n",
    "from model import TextCNN\n",
    "\n",
    "import sys\n",
    "\n",
    "# Parameters\n",
    "# ==================================================\n",
    "\n",
    "#reseting parameters\n",
    "import argparse as _argparse\n",
    "tf.flags._global_parser = _argparse.ArgumentParser()\n",
    "\n",
    "# Train data and epochs params\n",
    "tf.flags.DEFINE_string(\"data_folder\", \"./data\", \"Data source folder.\")\n",
    "tf.flags.DEFINE_integer(\"num_epochs\", 50, \"Number of training epochs (default: 50)\")\n",
    "\n",
    "# Data loading params\n",
    "tf.flags.DEFINE_float(\"dev_sample_percentage\", .1, \"Percentage of the training data to use for validation\")\n",
    "\n",
    "# Model Hyperparameters\n",
    "tf.flags.DEFINE_integer(\"embedding_dim\", 128, \"Dimensionality of character embedding (default: 128)\")\n",
    "tf.flags.DEFINE_string(\"filter_sizes\", \"2,3,4,5\", \"Comma-separated filter sizes (default: '3,4,5')\")\n",
    "tf.flags.DEFINE_integer(\"num_filters\", 128, \"Number of filters per filter size (default: 128)\")\n",
    "tf.flags.DEFINE_float(\"dropout_keep_prob\", 0.5, \"Dropout keep probability (default: 0.5)\")\n",
    "tf.flags.DEFINE_float(\"l2_reg_lambda\", 0.0, \"L2 regularization lambda (default: 0.0)\")\n",
    "\n",
    "# Training parameters\n",
    "tf.flags.DEFINE_integer(\"batch_size\", 64, \"Batch Size (default: 64)\")\n",
    "tf.flags.DEFINE_integer(\"evaluate_every\", 100, \"Evaluate model on dev set after this many steps (default: 100)\")\n",
    "tf.flags.DEFINE_integer(\"checkpoint_every\", 10, \"Save model after this many steps (default: 10)\")\n",
    "tf.flags.DEFINE_integer(\"num_checkpoints\", 5, \"Number of checkpoints to store (default: 5)\")\n",
    "\n",
    "# Misc Parameters\n",
    "tf.flags.DEFINE_boolean(\"allow_soft_placement\", True, \"Allow device soft device placement\")\n",
    "tf.flags.DEFINE_boolean(\"log_device_placement\", False, \"Log placement of ops on devices\")\n",
    "\n",
    "FLAGS = tf.flags.FLAGS\n",
    "FLAGS._parse_flags()\n",
    "\n",
    "print(\"\\nParameters:\")\n",
    "for attr, value in sorted(FLAGS.__flags.items()):\n",
    "    print(\"{}={}\".format(attr.upper(), value))\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile -a train.py\n",
    "# Data Preparation\n",
    "# ==================================================\n",
    "\n",
    "# Load data\n",
    "print(\"Loading data...\")\n",
    "categx, x_text, y = data_helpers.load_data_and_labels(FLAGS.data_folder)\n",
    "categx2int = {c: i for (i, c) in enumerate(categx)}\n",
    "\n",
    "# Testing new way of reading files\n",
    "# print(\"X:\")\n",
    "# print(x_text)\n",
    "# print(\"Y:\")\n",
    "# print(y)\n",
    "# sys.exit()\n",
    "\n",
    "# Build vocabulary\n",
    "max_document_length = max([len(x.split(\" \")) for x in x_text])\n",
    "vocab_processor = learn.preprocessing.VocabularyProcessor(max_document_length)\n",
    "x = np.array(list(vocab_processor.fit_transform(x_text)))\n",
    "\n",
    "# Extract word:id mapping from the object.\n",
    "vocab_dict_word_2_int = vocab_processor.vocabulary_._mapping\n",
    "vocab_dict_int_2_word = {i: w for w, i in vocab_dict_word_2_int.items()}\n",
    "\n",
    "# Randomly shuffle data\n",
    "np.random.seed(7)\n",
    "shuffle_indices = np.random.permutation(np.arange(len(y)))\n",
    "x_shuffled = x[shuffle_indices]\n",
    "y_shuffled = y[shuffle_indices]\n",
    "\n",
    "# Split train/test set\n",
    "# TODO: This is very crude, should use cross-validation\n",
    "dev_sample_index = -1 * int(FLAGS.dev_sample_percentage * float(len(y)))\n",
    "x_train, x_dev = x_shuffled[:dev_sample_index], x_shuffled[dev_sample_index:]\n",
    "y_train, y_dev = y_shuffled[:dev_sample_index], y_shuffled[dev_sample_index:]\n",
    "print(\"Vocabulary Size: {:d}\".format(len(vocab_processor.vocabulary_)))\n",
    "print(\"Train/Dev split: {:d}/{:d}\".format(len(y_train), len(y_dev)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile -a train.py\n",
    "\n",
    "# Training\n",
    "# ==================================================\n",
    "\n",
    "with tf.Graph().as_default():\n",
    "    session_conf = tf.ConfigProto(\n",
    "        allow_soft_placement=FLAGS.allow_soft_placement,\n",
    "        log_device_placement=FLAGS.log_device_placement)\n",
    "    sess = tf.Session(config=session_conf)\n",
    "    with sess.as_default():\n",
    "        cnn = TextCNN(\n",
    "            sequence_length=x_train.shape[1],\n",
    "            num_classes=y_train.shape[1],\n",
    "            vocab_size=len(vocab_processor.vocabulary_),\n",
    "            embedding_size=FLAGS.embedding_dim,\n",
    "            filter_sizes=list(map(int, FLAGS.filter_sizes.split(\",\"))),\n",
    "            num_filters=FLAGS.num_filters,\n",
    "            l2_reg_lambda=FLAGS.l2_reg_lambda)\n",
    "\n",
    "        # Define Training procedure\n",
    "        global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "        optimizer = tf.train.AdamOptimizer(1e-3)\n",
    "        grads_and_vars = optimizer.compute_gradients(cnn.loss)\n",
    "        train_op = optimizer.apply_gradients(grads_and_vars, global_step=global_step)\n",
    "\n",
    "        # Keep track of gradient values and sparsity (optional)\n",
    "        grad_summaries = []\n",
    "        for g, v in grads_and_vars:\n",
    "            if g is not None:\n",
    "                grad_hist_summary = tf.summary.histogram(\"{}/grad/hist\".format(v.name.replace(':','_')), g)\n",
    "                sparsity_summary = tf.summary.scalar(\"{}/grad/sparsity\".format(v.name.replace(':','_')), tf.nn.zero_fraction(g))\n",
    "                grad_summaries.append(grad_hist_summary)\n",
    "                grad_summaries.append(sparsity_summary)\n",
    "        grad_summaries_merged = tf.summary.merge(grad_summaries)\n",
    "\n",
    "        # Output directory for models and summaries\n",
    "        timestamp = str(int(time.time()))\n",
    "        current_dir = os.path.dirname(os.path.realpath(__file__))\n",
    "        out_dir = os.path.abspath(os.path.join(current_dir, \"runs\", timestamp))\n",
    "        print(\"Writing to {}\\n\".format(out_dir))\n",
    "\n",
    "        # Summaries for loss and accuracy\n",
    "        loss_summary = tf.summary.scalar(\"loss\", cnn.loss)\n",
    "        acc_summary = tf.summary.scalar(\"accuracy\", cnn.accuracy)\n",
    "\n",
    "        # Train Summaries\n",
    "        train_summary_op = tf.summary.merge([loss_summary, acc_summary, grad_summaries_merged])\n",
    "        train_summary_dir = os.path.join(out_dir, \"summaries\", \"train\")\n",
    "        train_summary_writer = tf.summary.FileWriter(train_summary_dir, sess.graph)\n",
    "\n",
    "        # Dev summaries\n",
    "        dev_summary_op = tf.summary.merge([loss_summary, acc_summary])\n",
    "        dev_summary_dir = os.path.join(out_dir, \"summaries\", \"dev\")\n",
    "        dev_summary_writer = tf.summary.FileWriter(dev_summary_dir, sess.graph)\n",
    "\n",
    "        # Checkpoint directory. Tensorflow assumes this directory already exists so we need to create it\n",
    "        checkpoint_dir = os.path.abspath(os.path.join(out_dir, \"checkpoints\"))\n",
    "        checkpoint_prefix = os.path.join(checkpoint_dir, \"model\")\n",
    "        if not os.path.exists(checkpoint_dir):\n",
    "            os.makedirs(checkpoint_dir)\n",
    "        saver = tf.train.Saver(tf.global_variables(), max_to_keep=FLAGS.num_checkpoints)\n",
    "\n",
    "        # Write vocabulary\n",
    "        vocab_processor.save(os.path.join(out_dir, \"vocab\"))\n",
    "\n",
    "        # Initialize all variables\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "\n",
    "        # Carrega o modelo já treinado anteriormente\n",
    "        # load_checkpoint_from = os.path.join(\".\\\\runs\\\\1499531641\\\\checkpoints\\\\\", 'model-1800')\n",
    "        # saver = tf.train.import_meta_graph(\"{}.meta\".format(load_checkpoint_from))\n",
    "        # saver.restore(sess, load_checkpoint_from)\n",
    "        # print(\"Loading checkpoint from {}\\n\".format(load_checkpoint_from))\n",
    "\n",
    "        def train_step(x_batch, y_batch):\n",
    "            \"\"\"\n",
    "            A single training step\n",
    "            \"\"\"\n",
    "            feed_dict = {\n",
    "                cnn.input_x: x_batch,\n",
    "                cnn.input_y: y_batch,\n",
    "                cnn.dropout_keep_prob: FLAGS.dropout_keep_prob\n",
    "            }\n",
    "            _, step, summaries, loss, accuracy = sess.run(\n",
    "                [train_op, global_step, train_summary_op, cnn.loss, cnn.accuracy],\n",
    "                feed_dict)\n",
    "            time_str = datetime.datetime.now().isoformat()\n",
    "            print(\"{}: step {}, loss {:g}, acc {:g}\".format(time_str, step, loss, accuracy))\n",
    "            train_summary_writer.add_summary(summaries, step)\n",
    "\n",
    "\n",
    "        def dev_step(x_batch, y_batch, writer=None):\n",
    "            \"\"\"\n",
    "            Evaluates model on a dev set\n",
    "            \"\"\"\n",
    "            feed_dict = {\n",
    "                cnn.input_x: x_batch,\n",
    "                cnn.input_y: y_batch,\n",
    "                cnn.dropout_keep_prob: 1.0\n",
    "            }\n",
    "            step, summaries, loss, accuracy, y_preds = sess.run(\n",
    "                [global_step, dev_summary_op, cnn.loss, cnn.accuracy, cnn.predictions],\n",
    "                feed_dict)\n",
    "            time_str = datetime.datetime.now().isoformat()\n",
    "            print(\"{}: step {}, loss {:g}, acc {:g}\".format(time_str, step, loss, accuracy))\n",
    "            # print(x_batch)\n",
    "            # print(y_batch)\n",
    "            # print(y_preds)\n",
    "            # print(x_batch.shape)\n",
    "            # print(vocab_dict_int_2_word)\n",
    "            # vocab_dict_word_2_int\n",
    "            for i in range(x_batch.shape[0]):\n",
    "                frase = \"\"\n",
    "                categoria_original = \"\"\n",
    "                categoria_predicao = \"\"\n",
    "                for w in range(x_batch.shape[1]):\n",
    "                    frase += vocab_dict_int_2_word[x_batch[i][w]] + \" \"\n",
    "                categoria_original = \"\".join([\"\" if a == 0 else b for (a, b) in zip(y_batch[i], categx)])\n",
    "\n",
    "                print(\"Sample: {}\\t -- categ_original: {}({})\\t categ_predicao: {}({})\".format(\n",
    "                    frase.encode(sys.stdout.encoding, errors='replace'), categx2int[categoria_original],\n",
    "                    categoria_original, y_preds[i], categx[y_preds[i]]))\n",
    "\n",
    "            if writer:\n",
    "                writer.add_summary(summaries, step)\n",
    "\n",
    "            print(\"{}: step {}, loss {:g}, acc {:g}\".format(time_str, step, loss, accuracy))\n",
    "\n",
    "\n",
    "        # Generate batches\n",
    "        batches = data_helpers.batch_iter(\n",
    "            list(zip(x_train, y_train)), FLAGS.batch_size, FLAGS.num_epochs)\n",
    "\n",
    "        print(\"Num Total Iters:\", (int((len(x_train) - 1) / FLAGS.batch_size) + 1) * FLAGS.num_epochs)\n",
    "\n",
    "        # Training loop. For each batch...\n",
    "        for batch in batches:\n",
    "            x_batch, y_batch = zip(*batch)\n",
    "            train_step(x_batch, y_batch)\n",
    "            current_step = tf.train.global_step(sess, global_step)\n",
    "            if current_step % FLAGS.evaluate_every == 0:\n",
    "                print(\"\\nEvaluation:\")\n",
    "                dev_step(x_dev, y_dev, writer=dev_summary_writer)\n",
    "                print(\"\")\n",
    "            if current_step % FLAGS.checkpoint_every == 0:\n",
    "                path = saver.save(sess, checkpoint_prefix, global_step=current_step)\n",
    "                print(\"Saved model checkpoint to {}\\n\".format(path))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Parameters:\n",
      "ALLOW_SOFT_PLACEMENT=True\n",
      "BATCH_SIZE=64\n",
      "CHECKPOINT_DIR=./runs/1516586601/checkpoints/\n",
      "CHECKPOINT_EVERY=10\n",
      "DATA_FOLDER=./data\n",
      "DEV_SAMPLE_PERCENTAGE=0.1\n",
      "DROPOUT_KEEP_PROB=0.5\n",
      "EMBEDDING_DIM=128\n",
      "EVAL_TRAIN=True\n",
      "EVALUATE_EVERY=100\n",
      "FILTER_SIZES=2,3,4,5\n",
      "L2_REG_LAMBDA=0.0\n",
      "LOG_DEVICE_PLACEMENT=False\n",
      "NUM_CHECKPOINTS=5\n",
      "NUM_EPOCHS=50\n",
      "NUM_FILTERS=128\n",
      "\n",
      "Loading data...\n",
      "Vocabulary Size: 86\n",
      "Train/Dev split: 34/3\n",
      "Writing to C:\\Users\\peter\\Projects\\NLU\\src\\runs\\1516587033\n",
      "\n",
      "Num Total Iters: 50\n",
      "2018-01-22T00:10:39.006212: step 1, loss 3.21829, acc 0.294118\n",
      "2018-01-22T00:10:39.061860: step 2, loss 2.08111, acc 0.382353\n",
      "2018-01-22T00:10:39.107481: step 3, loss 1.93059, acc 0.529412\n",
      "2018-01-22T00:10:39.148591: step 4, loss 0.957532, acc 0.676471\n",
      "2018-01-22T00:10:39.193715: step 5, loss 0.387543, acc 0.794118\n",
      "2018-01-22T00:10:39.234824: step 6, loss 0.910093, acc 0.705882\n",
      "2018-01-22T00:10:39.278948: step 7, loss 0.413751, acc 0.882353\n",
      "2018-01-22T00:10:39.330093: step 8, loss 0.51228, acc 0.882353\n",
      "2018-01-22T00:10:39.378217: step 9, loss 0.138297, acc 0.911765\n",
      "2018-01-22T00:10:39.424340: step 10, loss 0.142508, acc 0.941176\n",
      "Saved model checkpoint to C:\\Users\\peter\\Projects\\NLU\\src\\runs\\1516587033\\checkpoints\\model-10\n",
      "\n",
      "2018-01-22T00:10:40.560381: step 11, loss 0.164992, acc 0.911765\n",
      "2018-01-22T00:10:40.605311: step 12, loss 0.184083, acc 0.882353\n",
      "2018-01-22T00:10:40.649429: step 13, loss 0.0195037, acc 1\n",
      "2018-01-22T00:10:40.694551: step 14, loss 0.034933, acc 0.970588\n",
      "2018-01-22T00:10:40.740677: step 15, loss 0.144084, acc 0.970588\n",
      "2018-01-22T00:10:40.788806: step 16, loss 0.0147063, acc 1\n",
      "2018-01-22T00:10:40.841950: step 17, loss 0.0150218, acc 1\n",
      "2018-01-22T00:10:40.887072: step 18, loss 0.00545218, acc 1\n",
      "2018-01-22T00:10:40.930196: step 19, loss 0.197849, acc 0.941176\n",
      "2018-01-22T00:10:40.979831: step 20, loss 0.0244102, acc 0.970588\n",
      "Saved model checkpoint to C:\\Users\\peter\\Projects\\NLU\\src\\runs\\1516587033\\checkpoints\\model-20\n",
      "\n",
      "2018-01-22T00:10:42.210660: step 21, loss 0.0773169, acc 0.970588\n",
      "2018-01-22T00:10:42.257784: step 22, loss 0.117186, acc 0.970588\n",
      "2018-01-22T00:10:42.314941: step 23, loss 0.0123382, acc 1\n",
      "2018-01-22T00:10:42.366079: step 24, loss 0.00574858, acc 1\n",
      "2018-01-22T00:10:42.417219: step 25, loss 0.0225598, acc 1\n",
      "2018-01-22T00:10:42.472374: step 26, loss 0.0119133, acc 1\n",
      "2018-01-22T00:10:42.519504: step 27, loss 0.0133857, acc 1\n",
      "2018-01-22T00:10:42.567133: step 28, loss 0.00961905, acc 1\n",
      "2018-01-22T00:10:42.620780: step 29, loss 0.00172348, acc 1\n",
      "2018-01-22T00:10:42.683967: step 30, loss 0.0100481, acc 1\n",
      "Saved model checkpoint to C:\\Users\\peter\\Projects\\NLU\\src\\runs\\1516587033\\checkpoints\\model-30\n",
      "\n",
      "2018-01-22T00:10:43.922765: step 31, loss 0.00652739, acc 1\n",
      "2018-01-22T00:10:43.970893: step 32, loss 0.00785744, acc 1\n",
      "2018-01-22T00:10:44.021528: step 33, loss 0.0038875, acc 1\n",
      "2018-01-22T00:10:44.067654: step 34, loss 0.00479929, acc 1\n",
      "2018-01-22T00:10:44.115782: step 35, loss 0.0205984, acc 1\n",
      "2018-01-22T00:10:44.173943: step 36, loss 0.00135676, acc 1\n",
      "2018-01-22T00:10:44.227092: step 37, loss 0.0255504, acc 0.970588\n",
      "2018-01-22T00:10:44.301287: step 38, loss 0.0015529, acc 1\n",
      "2018-01-22T00:10:44.360950: step 39, loss 0.0017292, acc 1\n",
      "2018-01-22T00:10:44.439158: step 40, loss 0.000772756, acc 1\n",
      "Saved model checkpoint to C:\\Users\\peter\\Projects\\NLU\\src\\runs\\1516587033\\checkpoints\\model-40\n",
      "\n",
      "2018-01-22T00:10:45.838379: step 41, loss 0.00485852, acc 1\n",
      "2018-01-22T00:10:45.893024: step 42, loss 0.0413931, acc 0.970588\n",
      "2018-01-22T00:10:45.950176: step 43, loss 0.00280816, acc 1\n",
      "2018-01-22T00:10:46.006325: step 44, loss 0.00497861, acc 1\n",
      "2018-01-22T00:10:46.061973: step 45, loss 0.0619005, acc 0.970588\n",
      "2018-01-22T00:10:46.127147: step 46, loss 0.000740106, acc 1\n",
      "2018-01-22T00:10:46.176779: step 47, loss 0.00140887, acc 1\n",
      "2018-01-22T00:10:46.228416: step 48, loss 0.00766382, acc 1\n",
      "2018-01-22T00:10:46.280554: step 49, loss 0.00128406, acc 1\n",
      "2018-01-22T00:10:46.337205: step 50, loss 0.00796329, acc 1\n",
      "Saved model checkpoint to C:\\Users\\peter\\Projects\\NLU\\src\\runs\\1516587033\\checkpoints\\model-50\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%run -i train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting eval.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile eval.py\n",
    "# -*- coding: utf-8 -*-\n",
    "#!/usr/bin/env python\n",
    "\n",
    "# baseado no original em: https://github.com/dennybritz/cnn-text-classification-tf/blob/master/eval.py\n",
    "\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import learn\n",
    "import csv\n",
    "import sys\n",
    "\n",
    "\n",
    "import data_helpers\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Parameters\n",
    "# ==================================================\n",
    "\n",
    "#reseting parameters\n",
    "import argparse as _argparse\n",
    "tf.flags._global_parser = _argparse.ArgumentParser()\n",
    "\n",
    "\n",
    "# Data Parameters\n",
    "tf.flags.DEFINE_string(\"data_folder\", \"./data\", \"Data source folder.\")\n",
    "\n",
    "\n",
    "# Eval Parameters\n",
    "tf.flags.DEFINE_integer(\"batch_size\", 64, \"Batch Size (default: 64)\")\n",
    "tf.flags.DEFINE_string(\"checkpoint_dir\", \"\", \"Checkpoint directory from training run\")\n",
    "tf.flags.DEFINE_boolean(\"eval_train\", True, \"Evaluate on all training data\")\n",
    "tf.flags.DEFINE_string(\"x\", \"\", \"x (default: '')\")\n",
    "\n",
    "# Misc Parameters\n",
    "tf.flags.DEFINE_boolean(\"allow_soft_placement\", True, \"Allow device soft device placement\")\n",
    "tf.flags.DEFINE_boolean(\"log_device_placement\", False, \"Log placement of ops on devices\")\n",
    "\n",
    "# A variável FLAGs deve ser definida nos parâmetros de entrada do app\n",
    "# O comando executado deve conter o checkpoint desejado, como no exemplo:\n",
    "# python eval.py --eval_train --checkpoint_dir=\"./runs/1499500000/checkpoints/\"\n",
    "FLAGS = tf.flags.FLAGS\n",
    "FLAGS._parse_flags()\n",
    "\n",
    "# Caso FLAG não seja definido inicialmente\n",
    "# checkpoint_dir = último timestamp\n",
    "if FLAGS.checkpoint_dir == \"\":\n",
    "    print(\"Flag não declarada.\")\n",
    "    this_file_path = os.path.dirname(os.path.realpath(__file__))\n",
    "    runs = os.listdir(os.path.join(this_file_path, \"runs\"))\n",
    "    runs = [os.path.join(this_file_path+\"\\\\runs\", f) for f in runs]\n",
    "    runs.sort(key=lambda x: os.path.getmtime(x))\n",
    "    last_run = runs[-1]\n",
    "    print(\"Usando run '\"+str(last_run)+\"' no lugar.\")\n",
    "    checkpoint_file = os.path.join(this_file_path, \"runs\", last_run, \"checkpoints\")\n",
    "    print(checkpoint_file)\n",
    "    FLAGS.checkpoint_dir = checkpoint_file\n",
    "\n",
    "print(\"\\nParameters:\")\n",
    "for attr, value in sorted(FLAGS.__flags.items()):\n",
    "    print(\"{}={}\".format(attr.upper(), value))\n",
    "print(\"\")\n",
    "\n",
    "# CHANGE THIS: Load data. Load your own data here\n",
    "if FLAGS.eval_train:\n",
    "    categs_raw, x_raw, y_test = data_helpers.load_data_and_labels(FLAGS.data_folder)\n",
    "    y_test = np.argmax(y_test, axis=1)\n",
    "else:\n",
    "    #categs_raw = ['categ1', 'categ2']\n",
    "    #x_raw = [\"a masterpiece four years in the making\", \"everything is off.\"]\n",
    "    #y_test = [1, 0]\n",
    "    categs_raw, _, _ = data_helpers.load_data_and_labels(FLAGS.data_folder)\n",
    "    x_raw = [FLAGS.x]\n",
    "    y_test = [0]\n",
    "    \n",
    "categx2int = { c : i for (i, c) in enumerate(categs_raw)}\n",
    "\n",
    "\n",
    "# Map data into vocabulary\n",
    "vocab_path = os.path.join(FLAGS.checkpoint_dir, \"..\", \"vocab\")\n",
    "vocab_processor = learn.preprocessing.VocabularyProcessor.restore(vocab_path)\n",
    "x_test = np.array(list(vocab_processor.transform(x_raw)))\n",
    "\n",
    "# Extract word:id mapping from the object.\n",
    "vocab_dict_word_2_int = vocab_processor.vocabulary_._mapping\n",
    "vocab_dict_int_2_word = {i: w for w, i in vocab_dict_word_2_int.items()}\n",
    "\n",
    "print(\"\\nEvaluating...\\n\")\n",
    "\n",
    "# Evaluation\n",
    "# ==================================================\n",
    "checkpoint_file = tf.train.latest_checkpoint(FLAGS.checkpoint_dir)\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    session_conf = tf.ConfigProto(\n",
    "        allow_soft_placement=FLAGS.allow_soft_placement,\n",
    "        log_device_placement=FLAGS.log_device_placement)\n",
    "    sess = tf.Session(config=session_conf)\n",
    "    with sess.as_default():\n",
    "        # Load the saved meta graph and restore variables\n",
    "        saver = tf.train.import_meta_graph(\"{}.meta\".format(checkpoint_file))\n",
    "        saver.restore(sess, checkpoint_file)\n",
    "\n",
    "        # Get the placeholders from the graph by name\n",
    "        input_x = graph.get_operation_by_name(\"input_x\").outputs[0]\n",
    "        # input_y = graph.get_operation_by_name(\"input_y\").outputs[0]\n",
    "        dropout_keep_prob = graph.get_operation_by_name(\"dropout_keep_prob\").outputs[0]\n",
    "\n",
    "        # Tensors we want to evaluate\n",
    "        predictions = graph.get_operation_by_name(\"output/predictions\").outputs[0]\n",
    "\n",
    "        # Generate batches for one epoch\n",
    "        batches = data_helpers.batch_iter(list(zip(x_test, y_test)), FLAGS.batch_size, 1, shuffle=False)\n",
    "\n",
    "        # Collect the predictions here\n",
    "        all_predictions = []\n",
    "\n",
    "        for batch in batches:\n",
    "            x_test_batch, y_test_batch = zip(*batch)\n",
    "            batch_predictions = sess.run(predictions, {input_x: x_test_batch, dropout_keep_prob: 1.0})\n",
    "            all_predictions = np.concatenate([all_predictions, batch_predictions])\n",
    "            \n",
    "            #print(\"{}: step {}, loss {:g}, acc {:g}\".format(time_str, step, loss, accuracy))\n",
    "            #print(x_test_batch)\n",
    "            #print(y_test_batch)\n",
    "            #print(batch_predictions)\n",
    "            #print(x_batch.shape)\n",
    "            #print(vocab_dict_int_2_word)\n",
    "            #vocab_dict_word_2_int\n",
    "            for i in range(len(x_test_batch[:10])):\n",
    "\n",
    "                frase = \"\"\n",
    "                categoria_predicao = \"\"\n",
    "                for w in range(x_test_batch[0].shape[0]):\n",
    "                    frase += (vocab_dict_int_2_word[x_test_batch[i][w]] if vocab_dict_int_2_word[x_test_batch[i][\n",
    "                        w]] != \"<UNK>\" else \"\") + \" \"\n",
    "                categoria_original = categs_raw[y_test_batch[i]]\n",
    "                if FLAGS.eval_train:\n",
    "                    if categx2int[categoria_original] != batch_predictions[i]:\n",
    "                        print(\"FALSE MATCH: {}\\t -- original: {}({})\\t predicao: {}({})\".format(frase.encode(sys.stdout.encoding, errors='replace'),\n",
    "                            categx2int[categoria_original], categoria_original, batch_predictions[i], categs_raw[batch_predictions[i]]))\n",
    "                else:\n",
    "                    print(\"PREDICTION {}\\t predicao: {}({})\".format(frase.encode(sys.stdout.encoding, errors='replace'),\n",
    "                            batch_predictions[i], categs_raw[batch_predictions[i]]))\n",
    "                \n",
    "\n",
    "# Print accuracy if y_test is defined\n",
    "if y_test is not None:\n",
    "    correct_predictions = float(sum(all_predictions == y_test))\n",
    "    print(\"Total number of test examples: {}\".format(len(y_test)))\n",
    "    if FLAGS.eval_train:\n",
    "        print(\"Accuracy: {:g}\".format(correct_predictions / float(len(y_test))))\n",
    "\n",
    "# Save the evaluation to a csv\n",
    "predictions_human_readable = np.column_stack((np.array(x_raw), all_predictions))\n",
    "out_path = os.path.join(FLAGS.checkpoint_dir, \"..\", \"prediction.csv\")\n",
    "print(\"Saving evaluation to {0}\".format(out_path))\n",
    "\n",
    "import codecs\n",
    "with codecs.open(out_path, 'w', \"utf-8\") as f:\n",
    "    csv.writer(f).writerows(predictions_human_readable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Parameters:\n",
      "ALLOW_SOFT_PLACEMENT=True\n",
      "BATCH_SIZE=64\n",
      "CHECKPOINT_DIR=./runs/1516586601/checkpoints/\n",
      "DATA_FOLDER=./data\n",
      "EVAL_TRAIN=False\n",
      "LOG_DEVICE_PLACEMENT=False\n",
      "X=como faco um investimento?\n",
      "\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "INFO:tensorflow:Restoring parameters from C:\\Users\\peter\\Projects\\NLU\\src\\runs\\1516586601\\checkpoints\\model-10\n",
      "PREDICTION b'como faco um investimento           '\t predicao: 2(Informação Investimento.txt)\n",
      "Total number of test examples: 1\n",
      "Accuracy: 0\n",
      "Saving evaluation to ./runs/1516586601/checkpoints/..\\prediction.csv\n"
     ]
    }
   ],
   "source": [
    "%run -i eval.py --eval_train=False --checkpoint_dir=\"./runs/1516586601/checkpoints/\" --x=\"como faco um investimento?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Parameters:\n",
      "ALLOW_SOFT_PLACEMENT=True\n",
      "BATCH_SIZE=64\n",
      "CHECKPOINT_DIR=./runs/1516586601/checkpoints/\n",
      "CHECKPOINT_EVERY=10\n",
      "DATA_FOLDER=./data\n",
      "DEV_SAMPLE_PERCENTAGE=0.1\n",
      "DROPOUT_KEEP_PROB=0.5\n",
      "EMBEDDING_DIM=128\n",
      "EVAL_TRAIN=True\n",
      "EVALUATE_EVERY=100\n",
      "FILTER_SIZES=2,3,4,5\n",
      "L2_REG_LAMBDA=0.0\n",
      "LOG_DEVICE_PLACEMENT=False\n",
      "NUM_CHECKPOINTS=5\n",
      "NUM_EPOCHS=50\n",
      "NUM_FILTERS=128\n",
      "\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "INFO:tensorflow:Restoring parameters from C:\\Users\\peter\\Projects\\NLU\\src\\runs\\1516586601\\checkpoints\\model-10\n",
      "FALSE MATCH: b'mostra minha fatura            '\t -- original: 0(Consulta Fatura.txt)\t predicao: 2(Informação Investimento.txt)\n",
      "Total number of test examples: 37\n",
      "Accuracy: 0.972973\n",
      "Saving evaluation to ./runs/1516586601/checkpoints/..\\prediction.csv\n"
     ]
    }
   ],
   "source": [
    "%run -i eval.py --eval_train=True --checkpoint_dir=\"./runs/1516586601/checkpoints/\" --x=\"teste\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# %load ./runs/1516586601/checkpoints/..\\prediction.csv\n",
    "\"fatura digital , me mostra \\?\",0.0\n",
    "me mostra a fatura do cartao,0.0\n",
    "qria ver minha fatura do cartao,0.0\n",
    "mostra minha fatura \\?,2.0\n",
    "queo ver minha fatura digital,0.0\n",
    "\"pra ver a fatura , voce me mostra \\?\",0.0\n",
    "traz a fatura digital pra mim,0.0\n",
    "poe a fatura na tela \\?,0.0\n",
    "tem como ver mostrar fatura agora \\?,0.0\n",
    "quero ver minha fatura desse mes,0.0\n",
    "preciso ver meus gastos do mes passado na fatura digital,0.0\n",
    "quero ver os gastos do cartao agora,0.0\n",
    "mostra na tela os gastos no cartao \\?,0.0\n",
    "\"oie , gostaria de saber sobre a minha fatura digital pode me ajudar \\?\",1.0\n",
    "tem informacoes da fatura digital \\? onde posso encontrar \\?,1.0\n",
    "em que lugar do site consigo informacoes sobre quanto gastei no cartao \\?,1.0\n",
    "onde tem informacoes sobre o que gastei no cartao \\?,1.0\n",
    "tem mais informacoes sobre fatura digital \\?,1.0\n",
    "tem jeito de ver a fatura no site \\?,1.0\n",
    "pode mandar a fatura por e mail ou nao \\?,1.0\n",
    "como que faz pra saber mais sobre a fatutra do cartao \\?,1.0\n",
    "tem jeito de informar mais sobre a fatura digital,1.0\n",
    "como contrato o servico de fatura digital \\?,1.0\n",
    "onde encontro informacoes sobre a fatura do cartao \\?,1.0\n",
    "pode me falar mais sobre fatura digital \\?,1.0\n",
    "o que e a fatura digital \\?,1.0\n",
    "\"e os gastos do cartao , como faco pra ver \\?\",1.0\n",
    "onde invisto \\?,2.0\n",
    "como faco pra investir na poupanca \\?,2.0\n",
    "como investir em acoes \\?,2.0\n",
    "tem como investir em acoes \\?,2.0\n",
    "poupanca e um bom investimento \\?,2.0\n",
    "\"e se eu investir em titulos , quanto que rende \\?\",2.0\n",
    "rende muito investir em poupanca \\?,2.0\n",
    "qual o rendimento de titulos publicos \\?,2.0\n",
    "voce pode me explicar sobre investimentos \\?,2.0\n",
    "sabe de investimentos \\?,2.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
